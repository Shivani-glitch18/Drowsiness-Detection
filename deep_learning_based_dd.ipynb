{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49e50a6-5b18-45dc-81a1-5a0150c6bcfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import tensorflow as tf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df25bda-cb72-4de3-aa40-37abc169dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread(\"data/test/closed/s0001_00056_0_0_0_0_0_01.png\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112a5d3-7645-47b1-801e-23a6169cad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca596f-9326-47c9-be53-02b12d1e7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datadirectory = \"data/train/\"\n",
    "\n",
    "Classes = [\"closed\", \"open\"]\n",
    "\n",
    "for category in Classes:\n",
    "    path = os.path.join(Datadirectory, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "        backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "        plt.imshow(img_array, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990b552-8182-4c49-8122-3eb1a4cd79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "plt.imshow(new_array, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef433d9-0c60-48ab-b258-bdbc4962236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Data = []\n",
    "\n",
    "def create_training_Data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Datadirectory, category)\n",
    "        class_num = Classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "                new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "                training_Data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aace413-3777-49a7-9e47-50300ab53127",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_Data()\n",
    "print(len(training_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba591af-11ac-42dd-a7f9-f5fddf191881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b512bc9-f6c3-4a81-9ca3-7c7a244c4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features, label in training_Data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413c1ab-4a4d-47c8-b12c-15e11da94514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec23b6-dce7-4e5f-960e-aa1cda3aa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba0b7-8d94-4ee7-9ada-05184e6f3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539c13f-047b-4ed1-abbb-f83d0f939acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0628dbd-d4c1-4df6-9928-53a08d2f0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#later to load again directly\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba6b1d-e873-49a6-8f23-cda2d4fcda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f970f-4129-46cb-b5a1-adbcf69707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.mobilenet.MobileNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5510e8-9381-4f4e-a2a1-a341cf0f7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input = model.layers[0].input\n",
    "base_output = model.layers[-4].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796219a-25cb-4255-bf2e-c7010f0bcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flat_layer = layers.Flatten()(base_output)\n",
    "final_output = layers.Dense(1)(Flat_layer)  # one node (1/0)\n",
    "final_output = layers.Activation('sigmoid')(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7c556-191e-427c-86d4-13db4b5e7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.Model(inputs=base_input, outputs=final_output)\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f3d14-3ccf-4fd7-92c6-e8e8f034c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72d2e9-d702-4780-a96c-d61d5468af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(X, Y, epochs=25, validation_split=0.1)  # training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add5800-8822-42b3-b1b4-e417f0bf751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "new_model.save('my_model_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ddd04-41d6-4efa-9488-e618bd4a5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "new_model = tf.keras.models.load_model('my_model_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5107500-ce15-4003-8c0e-90ac88fafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the network for predictions\n",
    "img_array = cv2.imread(\"s0012_08255_0_0_1_1_0_02.png\", cv2.IMREAD_GRAYSCALE)\n",
    "backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "new_array = cv2.resize(backtorgb, (img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90caeb-b0a6-4514-833c-c6063622d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for prediction\n",
    "X_input = np.array(new_array).reshape(1, img_size, img_size, 3)\n",
    "X_input = X_input / 255.0  # Normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca0dbb-b2ba-4fd3-a070-22a8a0ddbaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resized image\n",
    "plt.imshow(new_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142e74a-dca0-4c9e-8ccf-342e8ef2cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the loaded model\n",
    "prediction = new_model.predict(X_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850971b2-ab11-46fa-99f0-dab14875a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sad_women.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d27a1a-31e0-4cf1-a6f4-978f94930005",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR3RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20002e34-afa6-46f7-8259-5c51324177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'harcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e93ef-7ba5-4948-8016-59eb91bec232",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'harcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195a932-bd1f-4138-9078-51c732757feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc227d-b753-46a6-bd2a-043e1b499264",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes = eye_cascade.detectMultiScale(gray, 1.1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd70bfb-d405-4d2b-a61b-897c4b5fe7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for(x, y, w, h) in eyes:\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e183d2e-b931-474f-83ce-87a907d5ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR3RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570ba59-882b-44a8-ac0b-f77d9dd2618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Haar cascade for eye detection\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Load the image\n",
    "#img = cv2.imread('path_to_your_image.jpg')  # Replace 'path_to_your_image.jpg' with the actual image path\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect eyes in the grayscale image\n",
    "eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "for x, y, w, h in eyes:\n",
    "    # Crop the region of interest (eye region)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Detect eyes in the cropped eye region\n",
    "    eyess = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "    if len(eyess) == 0:\n",
    "        print(\"Eyes are not detected\")\n",
    "    else:\n",
    "        for (ex, ey, ew, eh) in eyess:\n",
    "            # Crop the detected eye\n",
    "            eyes_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "\n",
    "# Display the cropped eye region\n",
    "plt.imshow(cv2.cvtColor(eyes_roi, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# Check the shape of the cropped eye\n",
    "print(\"Cropped Eye Shape:\", eyes_roi.shape)\n",
    "\n",
    "# Resize and preprocess the cropped eye for deep learning model input\n",
    "final_image = cv2.resize(eyes_roi, (224, 224))\n",
    "final_image = np.expand_dims(final_image, axis=0)  # Add batch dimension\n",
    "final_image = final_image / 255.0  # Normalize pixel values\n",
    "\n",
    "# Check the shape of the final preprocessed image\n",
    "print(\"Preprocessed Image Shape:\", final_image.shape)\n",
    "\n",
    "# Make predictions using the pre-trained model\n",
    "predictions = new_model.predict(final_image)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4f24d-3055-4c5c-9df0-c8db0198d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(path)\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(1)  # Use camera index 1, change it to 0 if needed\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)  # Try camera index 0 if index 1 failed\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read frame from the webcam\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display status (Open/Closed Eyes) on the frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    status = \"Open Eyes\" if len(faces) > 0 else \"Closed Eyes\"\n",
    "    cv2.putText(frame, status, (50, 50), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
    "\n",
    "    cv2.imshow('Drowsiness Detection Tutorial', frame)  # Display the frame\n",
    "\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()  # Release the webcam\n",
    "cv2.destroyAllWindows()  # Close all windows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
