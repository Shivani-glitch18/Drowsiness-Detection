{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665c9ee9-5001-42d2-87b8-8eb0fa21fb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54336 images belonging to 2 classes.\n",
      "Found 13583 images belonging to 2 classes.\n",
      "Found 16979 images belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajat\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.7047 - loss: 0.6633\n",
      "Epoch 1: val_loss improved from inf to 0.58483, saving model to C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\Project\\vgg19_model_checkpoint.keras\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8174s\u001b[0m 10s/step - accuracy: 0.7048 - loss: 0.6631 - val_accuracy: 0.7678 - val_loss: 0.5848 - learning_rate: 0.0010\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajat\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.58483 to 0.49746, saving model to C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\Project\\vgg19_model_checkpoint.keras\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.7333 - val_loss: 0.4975 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8427 - loss: 0.3663\n",
      "Epoch 3: val_loss did not improve from 0.49746\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7890s\u001b[0m 9s/step - accuracy: 0.8427 - loss: 0.3663 - val_accuracy: 0.7922 - val_loss: 0.5480 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.49746\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8000 - val_loss: 1.1202 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8679 - loss: 0.3187\n",
      "Epoch 5: val_loss did not improve from 0.49746\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7798s\u001b[0m 9s/step - accuracy: 0.8679 - loss: 0.3187 - val_accuracy: 0.8092 - val_loss: 0.5078 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.49746\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.6667 - val_loss: 0.7800 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8907 - loss: 0.2788\n",
      "Epoch 7: val_loss improved from 0.49746 to 0.47533, saving model to C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\Project\\vgg19_model_checkpoint.keras\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7811s\u001b[0m 9s/step - accuracy: 0.8907 - loss: 0.2788 - val_accuracy: 0.7944 - val_loss: 0.4753 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.47533 to 0.25924, saving model to C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\Project\\vgg19_model_checkpoint.keras\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9333 - val_loss: 0.2592 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8913 - loss: 0.2704\n",
      "Epoch 9: val_loss did not improve from 0.25924\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5425s\u001b[0m 6s/step - accuracy: 0.8913 - loss: 0.2704 - val_accuracy: 0.7997 - val_loss: 0.4731 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.25924\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.7333 - val_loss: 0.6293 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8943 - loss: 0.2680\n",
      "Epoch 11: val_loss did not improve from 0.25924\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2621s\u001b[0m 3s/step - accuracy: 0.8943 - loss: 0.2680 - val_accuracy: 0.8079 - val_loss: 0.4606 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.25924\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8000 - val_loss: 0.4173 - learning_rate: 1.0000e-05\n",
      "Epoch 13/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8976 - loss: 0.2579\n",
      "Epoch 13: val_loss did not improve from 0.25924\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2630s\u001b[0m 3s/step - accuracy: 0.8976 - loss: 0.2579 - val_accuracy: 0.8045 - val_loss: 0.4642 - learning_rate: 1.0000e-05\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.25924\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.7333 - val_loss: 0.3438 - learning_rate: 1.0000e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8979 - loss: 0.2588\n",
      "Epoch 15: val_loss did not improve from 0.25924\n",
      "\u001b[1m849/849\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2624s\u001b[0m 3s/step - accuracy: 0.8979 - loss: 0.2588 - val_accuracy: 0.8028 - val_loss: 0.4642 - learning_rate: 1.0000e-06\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 2s/step - accuracy: 0.8737 - loss: 0.2997\n",
      "Test Accuracy: 0.8730196356773376\n",
      "Test Loss: 0.3024420440196991\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 2s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      closed       0.50      0.58      0.54      8389\n",
      "        open       0.52      0.44      0.48      8590\n",
      "\n",
      "    accuracy                           0.51     16979\n",
      "   macro avg       0.51      0.51      0.51     16979\n",
      "weighted avg       0.51      0.51      0.51     16979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Data Augmentation Parameters\n",
    "batchsize = 64  # Increase batch size for faster training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, shear_range=0.2,\n",
    "                                   zoom_range=0.2, width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, validation_split=0.2)\n",
    "\n",
    "# Data Generators\n",
    "train_data = train_datagen.flow_from_directory(r'C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\data\\train',\n",
    "                                               target_size=(224, 224), batch_size=batchsize,\n",
    "                                               class_mode='categorical', subset='training')\n",
    "\n",
    "validation_data = train_datagen.flow_from_directory(r'C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\data\\train',\n",
    "                                                    target_size=(224, 224), batch_size=batchsize,\n",
    "                                                    class_mode='categorical', subset='validation')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(r'C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\data\\test',\n",
    "                                             target_size=(224, 224), batch_size=batchsize,\n",
    "                                             class_mode='categorical')\n",
    "\n",
    "# Base model\n",
    "base_model = VGG19(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "# Add custom head to the base model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Combined model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze layers in base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = r'C:\\Users\\rajat\\OneDrive\\Desktop\\drowsiness\\Project\\vgg19_model_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=7, verbose=1, restore_best_weights=True)\n",
    "\n",
    "learning_rate = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint, earlystop, learning_rate]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch=train_data.samples // batchsize,\n",
    "                    validation_data=validation_data,\n",
    "                    validation_steps=validation_data.samples // batchsize,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=30)\n",
    "\n",
    "# Load the best model\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss_test, acc_test = model.evaluate(test_data)\n",
    "\n",
    "# Print the test accuracy and loss\n",
    "print(\"Test Accuracy:\", acc_test)\n",
    "print(\"Test Loss:\", loss_test)\n",
    "\n",
    "# Get predictions for the test data\n",
    "predictions = model.predict(test_data)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map class indices to class labels\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "# Get true labels for the test data\n",
    "true_classes = test_data.classes\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064844d-e48c-43ef-97a7-84ae59f22b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
